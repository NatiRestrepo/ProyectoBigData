{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, csv, glob, json, uuid, pickle, math\n",
    "import nltk\n",
    "import numpy as np, scipy, pandas as pd\n",
    "from operator import itemgetter\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "from tqdm import tqdm\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/njaram15/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/njaram15/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = sc._gateway.jvm.java.net.URI\n",
    "Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "fs = FileSystem.get(URI(\"hdfs://hdpjupyter.dis.eafit.edu.co:8020/user/njaram15/\"), sc._jsc.hadoopConfiguration())\n",
    "\n",
    "# We can now use the Hadoop FileSystem API (https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html)\n",
    "#fs.listStatus(Path('/user/njaram15/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTENT_INDEX = 9\n",
    "\n",
    "userPath = '/user/njaram15/'\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "CONTENT_PATH = 'inputs/contents/'\n",
    "TOKENS_PATH = 'inputs/tokens/'\n",
    "CENTROIDS_PATH = 'inputs/centroids/'\n",
    "MODEL_PATH = 'model/'\n",
    "\n",
    "fs.mkdirs(Path(CONTENT_PATH))\n",
    "fs.mkdirs(Path(TOKENS_PATH))\n",
    "fs.mkdirs(Path(CENTROIDS_PATH))\n",
    "fs.mkdirs(Path(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(userPath+'/inputs/*.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "|_c0|   id|               title|   publication|              author|      date|  year|month| url|             content|\n",
      "+---+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "|  0|17283|House Republicans...|New York Times|          Carl Hulse|2016-12-31|2016.0| 12.0|null|WASHINGTON  —   C...|\n",
      "|  1|17284|Rift Between Offi...|New York Times|Benjamin Mueller ...|2017-06-19|2017.0|  6.0|null|After the bullet ...|\n",
      "|  2|17285|Tyrus Wong, ‘Bamb...|New York Times|        Margalit Fox|2017-01-06|2017.0|  1.0|null|When Walt Disney’...|\n",
      "|  3|17286|Among Deaths in 2...|New York Times|    William McDonald|2017-04-10|2017.0|  4.0|null|Death may be the ...|\n",
      "|  4|17287|Kim Jong-un Says ...|New York Times|       Choe Sang-Hun|2017-01-02|2017.0|  1.0|null|SEOUL, South Kore...|\n",
      "|  5|17288|Sick With a Cold,...|New York Times|         Sewell Chan|2017-01-02|2017.0|  1.0|null|LONDON  —   Queen...|\n",
      "|  6|17289|Taiwan’s Presiden...|New York Times| Javier C. Hernández|2017-01-02|2017.0|  1.0|null|BEIJING  —   Pres...|\n",
      "|  7|17290|After ‘The Bigges...|New York Times|         Gina Kolata|2017-02-08|2017.0|  2.0|null|Danny Cahill stoo...|\n",
      "|  8|17291|First, a Mixtape....|New York Times|    Katherine Rosman|2016-12-31|2016.0| 12.0|null|Just how   is Hil...|\n",
      "|  9|17292|Calling on Angels...|New York Times|         Andy Newman|2016-12-31|2016.0| 12.0|null|Angels are everyw...|\n",
      "| 10|17293|Weak Federal Powe...|New York Times|       Justin Gillis|2017-01-03|2017.0|  1.0|null|With Donald J. Tr...|\n",
      "| 11|17294|Can Carbon Captur...|New York Times|       John Schwartz|2017-01-05|2017.0|  1.0|null|THOMPSONS, Tex.  ...|\n",
      "| 12|17295|Mar-a-Lago, the F...|New York Times|     Maggie Haberman|2017-01-02|2017.0|  1.0|null|WEST PALM BEACH, ...|\n",
      "| 13|17296|How to form healt...|New York Times|      Charles Duhigg|2017-01-02|2017.0|  1.0|null|This article is p...|\n",
      "| 14|17297|Turning Your Vaca...|New York Times|Stephanie Rosenbloom|2017-04-14|2017.0|  4.0|null|It’s the season f...|\n",
      "| 15|17298|As Second Avenue ...|New York Times| Emma G. Fitzsimmons|2017-01-02|2017.0|  1.0|null|Finally. The Seco...|\n",
      "| 16|17300|Dylann Roof Himse...|New York Times|Kevin Sack and Al...|2017-01-02|2017.0|  1.0|null|  pages into the ...|\n",
      "| 17|17301|Modi’s Cash Ban B...|New York Times|         Geeta Anand|2017-01-02|2017.0|  1.0|null|MUMBAI, India  — ...|\n",
      "| 18|17302|Suicide Bombing i...|New York Times|The Associated Press|2017-01-03|2017.0|  1.0|null|BAGHDAD  —   A su...|\n",
      "| 19|17303|Fecal Pollution T...|New York Times|          Brett Cole|2017-01-03|2017.0|  1.0|null|SYDNEY, Australia...|\n",
      "+---+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON  —   Congressional Republicans have a new fear when it comes to their    health care lawsuit against the Obama administration: They might win. The incoming Trump administration could choose to no longer defend the executive branch against the suit, which challenges the administration’s authority to spend billions of dollars on health insurance subsidies for   and   Americans, handing House Republicans a big victory on    issues. But a sudden loss of the disputed subsidies could conceivably cause the health care program to implode, leaving millions of people without access to health insurance before Republicans have prepared a replacement. That could lead to chaos in the insurance market and spur a political backlash just as Republicans gain full control of the government. To stave off that outcome, Republicans could find themselves in the awkward position of appropriating huge sums to temporarily prop up the Obama health care law, angering conservative voters who have been demanding an end to the law for years. In another twist, Donald J. Trump’s administration, worried about preserving executive branch prerogatives, could choose to fight its Republican allies in the House on some central questions in the dispute. Eager to avoid an ugly political pileup, Republicans on Capitol Hill and the Trump transition team are gaming out how to handle the lawsuit, which, after the election, has been put in limbo until at least late February by the United States Court of Appeals for the District of Columbia Circuit. They are not yet ready to divulge their strategy. “Given that this pending litigation involves the Obama administration and Congress, it would be inappropriate to comment,” said Phillip J. Blando, a spokesman for the Trump transition effort. “Upon taking office, the Trump administration will evaluate this case and all related aspects of the Affordable Care Act. ” In a potentially   decision in 2015, Judge Rosemary M. Collyer ruled that House Republicans had the standing to sue the executive branch over a spending dispute and that the Obama administration had been distributing the health insurance subsidies, in violation of the Constitution, without approval from Congress. The Justice Department, confident that Judge Collyer’s decision would be reversed, quickly appealed, and the subsidies have remained in place during the appeal. In successfully seeking a temporary halt in the proceedings after Mr. Trump won, House Republicans last month told the court that they “and the  ’s transition team currently are discussing potential options for resolution of this matter, to take effect after the  ’s inauguration on Jan. 20, 2017. ” The suspension of the case, House lawyers said, will “provide the   and his future administration time to consider whether to continue prosecuting or to otherwise resolve this appeal. ” Republican leadership officials in the House acknowledge the possibility of “cascading effects” if the   payments, which have totaled an estimated $13 billion, are suddenly stopped. Insurers that receive the subsidies in exchange for paying    costs such as deductibles and   for eligible consumers could race to drop coverage since they would be losing money. Over all, the loss of the subsidies could destabilize the entire program and cause a lack of confidence that leads other insurers to seek a quick exit as well. Anticipating that the Trump administration might not be inclined to mount a vigorous fight against the House Republicans given the  ’s dim view of the health care law, a team of lawyers this month sought to intervene in the case on behalf of two participants in the health care program. In their request, the lawyers predicted that a deal between House Republicans and the new administration to dismiss or settle the case “will produce devastating consequences for the individuals who receive these reductions, as well as for the nation’s health insurance and health care systems generally. ” No matter what happens, House Republicans say, they want to prevail on two overarching concepts: the congressional power of the purse, and the right of Congress to sue the executive branch if it violates the Constitution regarding that spending power. House Republicans contend that Congress never appropriated the money for the subsidies, as required by the Constitution. In the suit, which was initially championed by John A. Boehner, the House speaker at the time, and later in House committee reports, Republicans asserted that the administration, desperate for the funding, had required the Treasury Department to provide it despite widespread internal skepticism that the spending was proper. The White House said that the spending was a permanent part of the law passed in 2010, and that no annual appropriation was required  —   even though the administration initially sought one. Just as important to House Republicans, Judge Collyer found that Congress had the standing to sue the White House on this issue  —   a ruling that many legal experts said was flawed  —   and they want that precedent to be set to restore congressional leverage over the executive branch. But on spending power and standing, the Trump administration may come under pressure from advocates of presidential authority to fight the House no matter their shared views on health care, since those precedents could have broad repercussions. It is a complicated set of dynamics illustrating how a quick legal victory for the House in the Trump era might come with costs that Republicans never anticipated when they took on the Obama White House.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for line in df.rdd.collect():\n",
    "    count = count + 1\n",
    "    content = line['content']\n",
    "    cname = CONTENT_PATH + str(count) + '.txt'\n",
    "    tname = TOKENS_PATH + str(count) + '.tokens'\n",
    "    #write_s3file(cname,content)\n",
    "    if not fs.exists(Path(userPath+cname)):\n",
    "        sc.parallelize([content]).saveAsTextFile(cname)    \n",
    "    sentences = \"\"\n",
    "    for sentence in nltk.sent_tokenize(content):\n",
    "        sentences = sentences + sentence.lower() + \"\\n\"\n",
    "\n",
    "    #write_s3file(tname, sentences)\n",
    "    if not fs.exists(Path(userPath+tname)):\n",
    "        sc.parallelize([sentences]).saveAsTextFile(tname)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def cleanStopWordAndStemming(sentences):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    sen = []\n",
    "    for sentence in sentences:\n",
    "        senten = [word for word in sentence if word not in stop_words]\n",
    "        singles = [stemmer.stem(token) for token in senten]\n",
    "        sen.append(singles)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "df2 = spark.read.text(userPath+'/inputs/tokens/*.tokens')\n",
    "for line in df2.collect():\n",
    "    sentences.append(nltk.word_tokenize(line['value'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joachim', 'neander', 'calvinist', 'theologian', 'often', 'hike', 'valley', 'outsid', 'düsseldorf', ',', 'germani', ',', 'write', 'hymn', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = cleanStopWordAndStemming(sentences)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|       word|              vector|\n",
      "+-----------+--------------------+\n",
      "|    rahmani|[-0.0513312593102...|\n",
      "|      brink|[0.15214513242244...|\n",
      "|   promenad|[0.16506512463092...|\n",
      "|    acronym|[0.06156763806939...|\n",
      "|  forgotten|[0.27298960089683...|\n",
      "|     justif|[-0.0942504927515...|\n",
      "|     teresa|[0.04786748439073...|\n",
      "|      lover|[0.08759211003780...|\n",
      "|     comedi|[0.03018949925899...|\n",
      "|  regularli|[0.25700578093528...|\n",
      "|      fanci|[0.01558598503470...|\n",
      "|       elit|[0.34433779120445...|\n",
      "|    speaker|[0.11496806144714...|\n",
      "|       chee|[0.13994784653186...|\n",
      "|       lion|[0.06953834742307...|\n",
      "|       rate|[0.30278375744819...|\n",
      "|     pepper|[0.08770198374986...|\n",
      "|       2014|[0.00547009333968...|\n",
      "|uncertainti|[0.31668323278427...|\n",
      "|nonetheless|[0.14262537658214...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = [(x,) for x in sentences]\n",
    "doc = spark.createDataFrame(inputs, [\"sentence\"])\n",
    "word2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=\"sentence\", outputCol=\"model\")\n",
    "model = word2Vec.fit(doc)\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fs.exists(Path(userPath+'/model/word2vec-model')):\n",
    "    model.save(userPath+'/model/word2vec-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = Word2VecModel.load(userPath+'/model/word2vec-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.getVectors().select('word').collect()\n",
    "vocab = dict([(v.word, k) for k, v in enumerate(words)])\n",
    "#write_s3file('taller2/model/w2v-lc-vocab.json', json.dumps(vocab))\n",
    "if not fs.exists(Path(userPath+'/model/w2v-lc-vocab.json')):\n",
    "    sc.parallelize([json.dumps(vocab)]).saveAsTextFile(userPath+'/model/w2v-lc-vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     word|similarity|\n",
      "+---------+----------+\n",
      "|    sport|   1.00000|\n",
      "|    chunk|   0.99398|\n",
      "|       57|   0.99322|\n",
      "|  worship|   0.99149|\n",
      "|   flight|   0.98690|\n",
      "|intercept|   0.98636|\n",
      "|    bride|   0.98564|\n",
      "|    exten|   0.98529|\n",
      "|  toddler|   0.98524|\n",
      "|    doubl|   0.98068|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number as fmt\n",
    "\n",
    "vectors = model.getVectors()\n",
    "t = vectors.filter(vectors.word == 'sport').select('vector').collect()[0]['vector']\n",
    "# como senate y alabama no están en el dataset de prueba se simulan con ceros\n",
    "s = np.array([0.0,0.0,0.0,0.0,0.0])\n",
    "a = np.array([0.0,0.0,0.0,0.0,0.0])\n",
    "r = t+s-a\n",
    "# r = a-t+s\n",
    "model.findSynonyms(r, 10).select(\"word\", fmt(\"similarity\", 5).alias(\"similarity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|            filename|\n",
      "+--------------------+--------------------+\n",
      "|Joachim Neander w...|hdfs://hdpjupyter...|\n",
      "|WASHINGTON  —   W...|hdfs://hdpjupyter...|\n",
      "|When Indira Islas...|hdfs://hdpjupyter...|\n",
      "|On the morning of...|hdfs://hdpjupyter...|\n",
      "|In the fall of 20...|hdfs://hdpjupyter...|\n",
      "|Hours before the ...|hdfs://hdpjupyter...|\n",
      "|President Obama d...|hdfs://hdpjupyter...|\n",
      "|One night six yea...|hdfs://hdpjupyter...|\n",
      "|After the bullet ...|hdfs://hdpjupyter...|\n",
      "|The Season 7 “Rea...|hdfs://hdpjupyter...|\n",
      "|On the night of N...|hdfs://hdpjupyter...|\n",
      "|WASHINGTON  —   O...|hdfs://hdpjupyter...|\n",
      "|Canada, our No. 1...|hdfs://hdpjupyter...|\n",
      "|A   of   lead exp...|hdfs://hdpjupyter...|\n",
      "|Updated: 11:50 p....|hdfs://hdpjupyter...|\n",
      "|• Hundreds of tho...|hdfs://hdpjupyter...|\n",
      "|Thousands of year...|hdfs://hdpjupyter...|\n",
      "|GLEN ELDER, Kan. ...|hdfs://hdpjupyter...|\n",
      "|Danny Cahill stoo...|hdfs://hdpjupyter...|\n",
      "|They called him t...|hdfs://hdpjupyter...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "df3 = spark.read.text(userPath+'/inputs/contents/*.txt')\n",
    "df3 = df3.withColumn(\"filename\", input_file_name())\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed:  1682.779007434845\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "dic = {k:v for k,v in vectors.collect()}\n",
    "for fname in df3.rdd.collect():\n",
    "    rdd = sc.parallelize([fname['value']])\n",
    "    centroid_in = list(rdd.map(lambda x: [dic[w] if w in dic.keys() else [0.0,0.0,0.0,0.0,0.0] for w in x.lower().split(' ')]).map(lambda x: np.mean(x, axis=0)).collect()[0])\n",
    "    out_dict = { fname['filename'] : centroid_in}\n",
    "    json_file = '/inputs/centroids/' + fname['filename'].split('/')[-2].replace('.txt', '.json')\n",
    "    if not fs.exists(Path(userPath+json_file)):\n",
    "        sc.parallelize([json.dumps(out_dict)]).saveAsTextFile(userPath+json_file)\n",
    "\n",
    "print('time elapsed: ', time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [(s,) for s in sentences]\n",
    "df4 = spark.createDataFrame(sents)\n",
    "dictionary = df4.rdd.map(lambda x: x[0]).reduce(lambda x,y: list(np.unique(list(x)+list(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "|{\"hdfs://hdpjupyt...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "centroid_dict = {}\n",
    "dfCentroide = spark.read.text(userPath+'inputs/centroids/*.json')\n",
    "dfCentroide.show()\n",
    "for fname in dfCentroide.rdd.collect():\n",
    "    d = json.loads(fname['value'])\n",
    "    centroid_dict.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_centroid_dict = {k: centroid_dict[k] for k in centroid_dict if not np.isnan(centroid_dict[k][0]).any()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(x, out=False):\n",
    "    if x in dictionary:\n",
    "        return vectors.filter(vectors.word == x).select('vector').collect()[0]['vector']\n",
    "    else:\n",
    "        return np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document(q_embeddings, d_centroid):\n",
    "    individual_csims = [(1 - scipy.spatial.distance.cosine(qin, d_centroid)) for qin in q_embeddings]\n",
    "    return (sum(individual_csims)/len(q_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "df5 = spark.read.text(userPath+'/inputs/contents/*.txt')\n",
    "for fname in df5.rdd.collect():\n",
    "    documents.append(nltk.word_tokenize(fname['value'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words in query:  2 Num query word in vectors:  2\n"
     ]
    }
   ],
   "source": [
    "query = 'mother buys'\n",
    "query_words = nltk.word_tokenize(query.lower())\n",
    "query_ins = [get_embedding(x) for x in query_words]\n",
    "q_len = len(query_ins)\n",
    "print('Num words in query: ', len(query_words), 'Num query word in vectors: ', q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "scores_in_in = []\n",
    "for k,v in clean_centroid_dict.items():\n",
    "    scores_in_in.append((k, score_document(query_ins, v[0])))\n",
    "\n",
    "scores_in_in = sorted(scores_in_in, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5 IN-IN:\n",
      "hdfs://hdpjupyter.dis.eafit.edu.co:8020/user/njaram15/inputs/contents/324.txt/part-00001\n",
      "hdfs://hdpjupyter.dis.eafit.edu.co:8020/user/njaram15/inputs/contents/116.txt/part-00001\n",
      "hdfs://hdpjupyter.dis.eafit.edu.co:8020/user/njaram15/inputs/contents/280.txt/part-00001\n",
      "hdfs://hdpjupyter.dis.eafit.edu.co:8020/user/njaram15/inputs/contents/283.txt/part-00001\n",
      "hdfs://hdpjupyter.dis.eafit.edu.co:8020/user/njaram15/inputs/contents/452.txt/part-00001\n"
     ]
    }
   ],
   "source": [
    "print('TOP 5 IN-IN:')\n",
    "top_5_in_in = [x[0] for x in scores_in_in[:5]]\n",
    "\n",
    "for fname in top_5_in_in:\n",
    "    print(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
